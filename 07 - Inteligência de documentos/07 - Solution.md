# ðŸ“– SoluÃ§Ã£o: AutomaÃ§Ã£o do Processamento de Documentos com Logic Apps

## ðŸ”¹ Objetivo

Neste desafio vocÃª irÃ¡:

âœ… Configurar uma **Azure Logic App** para processar PDFs automaticamente
âœ… Habilitar Managed Identity e atribuir permissÃµes Ã  Logic App
âœ… Usar **Document Intelligence (Form Recognizer)** para analisar PDFs
âœ… Criar um pipeline ADF para mover PDFs do Fabric para uma Storage Account
âœ… Armazenar saÃ­das JSON analisadas na Storage Account
âœ… Validar que os JSONs sejam gravados corretamente

---

## ðŸš€ Passo 1: Criar Logic App

1. No Azure Portal, vÃ¡ em **Logic Apps** â†’ **+ Create**.
2. Configure Resource Group, regiÃ£o e nome (`YourLogicApp`).
3. Selecione **Plan Type**: `Consumption`.
4. Review + Create â†’ Create.

---

## ðŸš€ Passo 2: Habilitar Managed Identity

1. Abra **YourLogicApp** â†’ **Identity** â†’ habilite **System Assigned Identity**.
2. Salve e copie o **Object (Principal) ID**.

---

## ðŸš€ Passo 3: Atribuir permissÃµes

- No Storage Account e no recurso de Document Intelligence â†’ **Access Control (IAM)**.
- Role assignments para a Managed Identity:
  - **Storage Blob Data Contributor**
  - **Storage Account Contributor**
  - **Cognitive Services Contributor**

---

## ðŸš€ Passo 4: Configurar Logic App (Designer)

- Trigger: **When a blob is added or modified (properties only)**
  - Storage Account: `Your Storage Account`
  - Container: `incoming-docs`
  - Trigger conditions: `Ends with .pdf`

- Step: **Analyze Document (Prebuilt-Invoice)**
  - Storage URL: `https://yourstoragename.blob.core.windows.net/incoming-docs/@{triggerOutputs()?['body']['name']}`

- Step: **Create Blob**
  - Container: `processed-json`
  - Blob Name: `analyzed-document-@{triggerOutputs()?['body']['name']}.json`
  - Blob Content: `@body('Analyze_Document_for_Prebuilt_or_Custom_models_(v4.x_API)')`

---

## ðŸš€ Passo 5: (Opcional) Usar JSON do workflow

Cole o JSON de definiÃ§Ã£o no Code View do Logic App para acelerar a configuraÃ§Ã£o.

---

## ðŸš€ Passo 6: Criar Azure Data Factory (ADF)

1. No Azure Portal â†’ Data Factory â†’ + Create.
2. Configure subscription, resource group, regiÃ£o e nome (`YourADFInstance`).

---

## ðŸš€ Passo 7: Criar pipeline Copy no ADF

- Source: Microsoft Fabric Lakehouse Files (Service Principal)
- Sink: Azure Blob Storage (container para os JSONs)
- Validate â†’ Trigger Now â†’ Publish

---

## ðŸš€ Passo 8: Verificar JSONs em Storage

- No Azure Portal â†’ Storage Account â†’ `processed-json` â†’ verificar arquivos JSON.

---

## ðŸš€ Passo 9: Criar Lakehouse em Fabric para os JSONs analisados

- No Microsoft Fabric â†’ Workspace â†’ + New Item â†’ criar novo Lakehouse para dados analisados.

---

## ðŸŽ¯ Passo 10: Challenge

- Importar os JSONs analisados para seu Lakehouse no Fabric.

---

## âœ… Resultado final
Os documentos processados sÃ£o salvos como JSON na Storage Account e estÃ£o prontos para serem consumidos em anÃ¡lises dentro do Fabric.
