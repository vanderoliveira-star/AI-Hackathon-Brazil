# **Desafio 02 â€“ TransformaÃ§Ã£o intermediÃ¡ria e anÃ¡lise exploratÃ³ria (Silver) ğŸ”§ğŸ“Š**

## **Objetivo e soluÃ§Ã£o passo a passo ğŸ§­**

### **Objetivo ğŸ¯**
Transformar os dados Bronze e realizar anÃ¡lise exploratÃ³ria em Silver.

---

## **SoluÃ§Ã£o passo a passo ğŸªœ**

### **Conjunto de score de crÃ©dito e conjunto de produtos ğŸ§©**

- Criar novo Dataflow Gen2
- Configurar a origem na tabela Bronze.
- Aplicar transformaÃ§Ãµes intermediÃ¡rias
- Criar coluna de score estimado
- Segmentar clientes por perfil de crÃ©dito
- Identificar o cluster com maior mÃ©dia de score
- Filtrar os clientes pertencentes a esse cluster
- Resultado: subconjunto de clientes com perfil de crÃ©dito alto

---

# **SegmentaÃ§Ã£o de clientes por score creditÃ­cio ğŸ§®**

Importar funÃ§Ãµes necessÃ¡rias

```python
from pyspark.sql.functions import col, when, udf
from pyspark.sql.types import StringType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
```


## **Carregar tabela Silver com dados financeiros ğŸ§¾**

```python
df_fin = spark.read.table("creditScore_silver")
```

---

## **Derivar coluna score_estimado baseada em comportamento de pagamento e uso de crÃ©dito ğŸ’³**

```python
df_fin = df_fin.withColumn("score_estimado",
    when(col("Payment_Behaviour") == "High_spent_Small_value_payments", 650)
    .when(col("Payment_Behaviour") == "Low_spent_Large_value_payments", 750)
    .when(col("Payment_Behaviour") == "High_spent_Large_value_payments", 800)
    .when(col("Payment_Behaviour") == "Low_spent_Small_value_payments", 600)
    .otherwise(620)
)
```

---

## **PenalizaÃ§Ã£o por pagamentos em atraso â°**

```python
df_fin = df_fin.withColumn("score_estimado",
    col("score_estimado") - (col("Num_of_Delayed_Payment") * 5)
)
```

---

## **PenalizaÃ§Ã£o por alto uso de crÃ©dito ğŸ“‰**

```python
df_fin = df_fin.withColumn("score_estimado",
    when(col("Credit_Utilization_Ratio") > 0.8, col("score_estimado") - 20)
    .otherwise(col("score_estimado"))
)
```

---

## **Limitar score entre 300 e 850 âš™ï¸**

```python
df_fin = df_fin.withColumn("score_estimado",
    when(col("score_estimado") < 300, 300)
    .when(col("score_estimado") > 850, 850)
    .otherwise(col("score_estimado"))
)
```

---

## **Filtrar registros vÃ¡lidos para clustering ğŸ§¹**

```python
df_fin_clean = df_fin.filter(col("score_estimado").isNotNull())
```

---

## **Vectorizar coluna score_estimado para ML ğŸ¤–**

```python
assembler = VectorAssembler(inputCols=["score_estimado"], outputCol="features")
df_fin_vec = assembler.transform(df_fin_clean)
```

---

## **Aplicar KMeans clustering para segmentar clientes ğŸ§ **

```python
kmeans = KMeans(k=3, seed=42)
model_fin = kmeans.fit(df_fin_vec)
df_fin_clustered = model_fin.transform(df_fin_vec)
```

---

## **Etiquetar perfis creditÃ­cios conforme mÃ©dia de score por cluster ğŸ·ï¸**

```python
cluster_scores = df_fin_clustered.groupBy("prediction") \
    .avg("score_estimado") \
    .orderBy("avg(score_estimado)", ascending=False) \
    .collect()
```

---

## **Criar mapa de etiquetas: Alto, MÃ©dio, Baixo ğŸ—ºï¸**

```python
cluster_map = {}
for i, row in enumerate(cluster_scores):
    cluster_map[row["prediction"]] = ["Alto", "MÃ©dio", "Baixo"][i]
```

---

## **UDF para atribuir etiqueta âš¡**

```python
def map_cluster(pred):
    return cluster_map.get(pred, "Desconhecido")

map_udf = udf(map_cluster, StringType())
df_segmentado = df_fin_clustered.withColumn("perfil_crediticio", map_udf(col("prediction")))
```

---

## **Contar clientes por perfil (opcional para validaÃ§Ã£o) ğŸ“Š**

```python
df_segmentado.groupBy("perfil_crediticio").count().orderBy("count", ascending=False).show()
```

---

## **Filtrar clientes com perfil Alto ğŸ¥‡**

```python
df_gold_fin = df_segmentado.filter(col("perfil_crediticio") == "Alto")
```

---

## **Salvar tabela Gold com clientes de melhor perfil creditÃ­cio ğŸ’¾**

```python
df_gold_fin.write.option("mergeSchema", "true").mode("overwrite").saveAsTable("creditScore_gold")
```

---




# **SegmentaÃ§Ã£o ML + PromoÃ§Ã£o a Gold (Retail por produto) ğŸ›ï¸ğŸ¤–**

---

## **ğŸ“Œ Importar funÃ§Ãµes necessÃ¡rias**

```python
from pyspark.sql.functions import col, when, udf
from pyspark.sql.types import StringType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
```

---

## **Carregar tabela Silver com catÃ¡logo de produtos retail ğŸ§¾**

```python
df_retail = spark.read.table("productos_silver")
```

---

## **ğŸ§® Derivar coluna valor_comercial = Price Ã— Stock**

```python
df_retail = df_retail.withColumn("valor_comercial", col("Price") * col("Stock"))
```

---

## **ğŸ§® Derivar coluna disponibilidade_binaria**

```python
df_retail = df_retail.withColumn("disponible",
    when(col("Availability") == "InStock", 1).otherwise(0)
)
```

---

## **ğŸ§¹ Filtrar registros vÃ¡lidos para clustering**

```python
df_retail_clean = df_retail.filter(
    col("valor_comercial").isNotNull() & col("disponible").isNotNull()
)
```

---

## **ğŸ“Š Vectorizar colunas para ML**

```python
assembler = VectorAssembler(inputCols=["valor_comercial", "disponible"], outputCol="features")
df_retail_vec = assembler.transform(df_retail_clean)
```

---

## **ğŸ¤– Aplicar KMeans clustering para segmentar produtos**

```python
kmeans = KMeans(k=3, seed=42)
model_retail = kmeans.fit(df_retail_vec)
df_retail_clustered = model_retail.transform(df_retail_vec)
```

---

## **ğŸ·ï¸ Etiquetar produtos conforme valor comercial mÃ©dio por cluster**

```python
cluster_scores = df_retail_clustered.groupBy("prediction") \
    .avg("valor_comercial") \
    .orderBy("avg(valor_comercial)", ascending=False) \
    .collect()
```

---

## **Criar mapa de etiquetas: Valioso, MÃ©dio, Baixo ğŸ—ºï¸**

```python
cluster_map = {}
for i, row in enumerate(cluster_scores):
    cluster_map[row["prediction"]] = ["Valioso", "MÃ©dio", "Baixo"][i]
```

---

## **UDF para atribuir etiqueta âš¡**

```python
def map_cluster(pred):
    return cluster_map.get(pred, "Desconhecido")

map_udf = udf(map_cluster, StringType())
df_segmentado = df_retail_clustered.withColumn("perfil_produto", map_udf(col("prediction")))
```

---

## **ğŸ” Contagem por perfil (opcional para validaÃ§Ã£o)**

```python
df_segmentado.groupBy("perfil_produto").count().orderBy("count", ascending=False).show()
```

---

## **ğŸ¥‡ Filtrar produtos valiosos e disponÃ­veis**

```python
df_gold_retail = df_segmentado.filter(
    (col("perfil_produto") == "Valioso") & (col("disponible") == 1)
)
```

---

## **ğŸ’¾ Salvar tabela Gold com produtos valiosos**

```python
df_gold_retail.write.option("mergeSchema", "true").mode("overwrite").saveAsTable("productos_gold")
```
